{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfef6fb",
   "metadata": {},
   "source": [
    "# ***Rigorous Data Cleaning***\n",
    "\n",
    "### Process Highlights:\n",
    "- Customer Data: direct standardization of dates, flagged NULL columns as missing in a separate column before replacing with \"unknown\"\n",
    "- Usage Logs: flagged missing European September logs before forward filling with August observations, created flags for daily login data\n",
    "- Support Tickets: categorized tickets and flagged by issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb039b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dbcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407670b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c809b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = pd.read_csv('../data/dataset1.csv')\n",
    "df_usage_q1q2 = pd.read_csv('../data/dataset2a for q1 q2.csv')\n",
    "df_usage_q3q4 = pd.read_csv('../data/dataset2b for q3 q4.csv')\n",
    "df_tickets = pd.read_csv('../data/dataset3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f049c24",
   "metadata": {},
   "source": [
    "### Customer Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c287ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix corrupted company size buckets\n",
    "size_map = {\n",
    "    '10-Jan': '1-10',\n",
    "    'Nov-50': '11-50'\n",
    "}\n",
    "df_customers['company_size_bucket'] = df_customers['company_size_bucket'].replace(size_map)\n",
    "\n",
    "# Flag missing industry entries prior to forward filling\n",
    "df_customers['industry_missing'] = df_customers['industry'].isna()\n",
    "\n",
    "# Fill missing industry with 'Unknown'\n",
    "df_customers['industry'] = df_customers['industry'].fillna('Unknown')\n",
    "\n",
    "# Convert dates\n",
    "df_customers['contract_start_date'] = pd.to_datetime(df_customers['contract_start_date'])\n",
    "df_customers['contract_end_date'] = pd.to_datetime(df_customers['contract_end_date'])\n",
    "\n",
    "# Create time-based features\n",
    "df_customers['contract_duration_days'] = (\n",
    "    df_customers['contract_end_date'] - df_customers['contract_start_date']\n",
    ").dt.days\n",
    "df_customers['acquisition_quarter'] = df_customers['contract_start_date'].dt.to_period('Q')\n",
    "df_customers['acquisition_month'] = df_customers['contract_start_date'].dt.to_period('M')\n",
    "\n",
    "# Check for invalid dates\n",
    "invalid_dates = df_customers[df_customers['contract_end_date'] < df_customers['contract_start_date']]\n",
    "if len(invalid_dates) > 0:\n",
    "    print(f\"WARNING: {len(invalid_dates)} records have end_date before start_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9c988",
   "metadata": {},
   "source": [
    "### Usage Logs Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7b94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "df_usage = pd.concat([df_usage_q1q2, df_usage_q3q4], ignore_index=True)\n",
    "\n",
    "# Convert dates to datetime\n",
    "df_usage['date'] = pd.to_datetime(df_usage['date'])\n",
    "\n",
    "# Handle corrupted EU data by sorting by customer and date \n",
    "df_usage = df_usage.sort_values(['customer_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Merge with customer data to get region information for EU corruption flagging\n",
    "df_usage = df_usage.merge(\n",
    "    df_customers[['customer_id', 'region', 'is_eu']], \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check for any usage records without matching customer\n",
    "missing_region = df_usage['region'].isna().sum()\n",
    "if missing_region > 0:\n",
    "    print(f\"WARNING: {missing_region} usage records have no matching customer\")\n",
    "\n",
    "# Flag corrupted EU September data\n",
    "df_usage['is_eu_sept_corrupted'] = (\n",
    "    (df_usage['is_eu'] == 1) & \n",
    "    (df_usage['date'].dt.month == 9) &\n",
    "    (df_usage['logins'].isna() | df_usage['feature_events'].isna() | df_usage['session_minutes'].isna())\n",
    ")\n",
    "corrupted_count = df_usage['is_eu_sept_corrupted'].sum()\n",
    "total_eu_sept = ((df_usage['is_eu'] == 1) & (df_usage['date'].dt.month == 9)).sum()\n",
    "\n",
    "# Forward fill corrupted records\n",
    "df_usage['logins'] = df_usage.groupby('customer_id')['logins'].ffill()\n",
    "df_usage['feature_events'] = df_usage.groupby('customer_id')['feature_events'].ffill()\n",
    "df_usage['session_minutes'] = df_usage.groupby('customer_id')['session_minutes'].ffill()\n",
    "\n",
    "# Create separate metrics without corrupted data\n",
    "df_usage_clean = df_usage[~df_usage['is_eu_sept_corrupted']].copy()\n",
    "\n",
    "usage_summary = df_usage.groupby('customer_id').agg({\n",
    "    'date': 'count', \n",
    "    'logins': ['sum', 'mean', 'std'],\n",
    "    'feature_events': ['sum', 'mean'],\n",
    "    'session_minutes': ['sum', 'mean'],\n",
    "    'is_eu_sept_corrupted': 'sum' \n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "usage_summary.columns = [\n",
    "    'customer_id', \n",
    "    'active_days', \n",
    "    'total_logins', 'avg_logins_per_day', 'std_logins',\n",
    "    'total_feature_events', 'avg_feature_events',\n",
    "    'total_session_minutes', 'avg_session_minutes',\n",
    "    'corrupted_records_count'\n",
    "]\n",
    "\n",
    "# Create clean metrics without corrupted data\n",
    "usage_summary_clean = df_usage_clean.groupby('customer_id').agg({\n",
    "    'date': 'count',\n",
    "    'logins': ['sum', 'mean'],\n",
    "    'session_minutes': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "usage_summary_clean.columns = [\n",
    "    'customer_id',\n",
    "    'active_days_clean',\n",
    "    'total_logins_clean', 'avg_logins_per_day_clean',\n",
    "    'total_session_minutes_clean'\n",
    "]\n",
    "\n",
    "# Merge clean metrics into main summary\n",
    "usage_summary = usage_summary.merge(usage_summary_clean, on='customer_id', how='left')\n",
    "\n",
    "# Flag customers with no logins or low engagement\n",
    "usage_summary['never_logged_in'] = usage_summary['total_logins'] == 0\n",
    "usage_summary['low_engagement'] = usage_summary['avg_logins_per_day'] < 1\n",
    "\n",
    "# Detect outliers based on 99th percentile\n",
    "outlier_threshold = usage_summary['total_logins'].quantile(0.99)\n",
    "usage_summary['potential_outlier'] = usage_summary['total_logins'] > outlier_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d3542",
   "metadata": {},
   "source": [
    "### Support Tickets Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622c2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ticket creation column to datetime\n",
    "df_tickets['created_at'] = pd.to_datetime(df_tickets['created_at'])\n",
    "\n",
    "# Extract month and quarter from created_at for time-series analysis\n",
    "df_tickets['ticket_month'] = df_tickets['created_at'].dt.to_period('M')\n",
    "df_tickets['ticket_quarter'] = df_tickets['created_at'].dt.to_period('Q')\n",
    "\n",
    "# Create flags for key issue types (based on ticket_text in dataset3)\n",
    "# Dashboard Issues\n",
    "dashboard_texts = {\n",
    "    \"Dashboard loads very slowly during peak hours.\",\n",
    "    \"The new dashboard is confusing for my staff.\"\n",
    "}\n",
    "df_tickets['is_dashboard_issue'] = df_tickets['ticket_text'].isin(dashboard_texts)\n",
    "\n",
    "# Product Issues\n",
    "product_texts = {\n",
    "    \"Inventory sync takes too long to complete.\",\n",
    "    \"The reporting page keeps timing out.\",\n",
    "    \"Mobile app crashes when I try to view inventory levels.\",\n",
    "    \"It's hard to find where to update product SKUs.\",\n",
    "    \"Bulk upload flow is not intuitive.\",\n",
    "    \"Search results are not clearly sorted.\"\n",
    "}\n",
    "df_tickets['is_product_bug'] = df_tickets['ticket_text'].isin(product_texts)\n",
    "\n",
    "# Onboarding Issues\n",
    "onboarding_texts = {\n",
    "    \"Sales promised a custom report that doesn't exist.\",\n",
    "    \"We expected multi-warehouse support out of the box.\",\n",
    "    \"Pricing discussed in the demo does not match our invoice.\",\n",
    "    \"We were told onboarding would be fully managed, but it wasn't.\",\n",
    "    \"VAT appears to be calculated incorrectly.\",\n",
    "    \"We were charged for extra users we don't have.\",\n",
    "    \"Our discount was not applied to this renewal.\",\n",
    "    \"I need to change our billing email and can't find the option.\"\n",
    "}\n",
    "df_tickets['is_onboarding_issue'] = df_tickets['ticket_text'].isin(onboarding_texts)\n",
    "\n",
    "# Calculate tickets per customer\n",
    "ticket_summary = df_tickets.groupby('customer_id').agg({\n",
    "    'ticket_id': 'count',\n",
    "    'is_dashboard_issue': 'sum',\n",
    "    'is_product_bug': 'sum',\n",
    "    'is_onboarding_issue': 'sum',\n",
    "    'sentiment': 'mean',\n",
    "    'first_response_hours': 'mean',\n",
    "    'resolution_hours': 'mean',\n",
    "    'resolved': 'mean'  \n",
    "}).reset_index()\n",
    "\n",
    "ticket_summary.columns = [\n",
    "    'customer_id', \n",
    "    'total_tickets',\n",
    "    'dashboard_issue_count',\n",
    "    'product_bug_count',\n",
    "    'onboarding_issue_count',\n",
    "    'avg_sentiment',\n",
    "    'avg_first_response_hours',\n",
    "    'avg_resolution_hours',\n",
    "    'resolution_rate'\n",
    "]\n",
    "\n",
    "# Flag high-ticket customers\n",
    "ticket_summary['high_ticket_volume'] = ticket_summary['total_tickets'] > ticket_summary['total_tickets'].quantile(0.75)\n",
    "\n",
    "# Determine primary issue type\n",
    "def get_primary_complaint(row):\n",
    "    if row['dashboard_issue_count'] > 0:\n",
    "        return 'Dashboard_Performance'\n",
    "    elif row['product_bug_count'] > 0:\n",
    "        return 'Product_Bug'\n",
    "    elif row['onboarding_issue_count'] > 0:\n",
    "        return 'Onboarding'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "ticket_summary['primary_complaint_type'] = ticket_summary.apply(get_primary_complaint, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf0fea",
   "metadata": {},
   "source": [
    "### Merge Datasets into Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195094d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/xrsm2sl9281d5zgcwfk59qbm0000gn/T/ipykernel_2690/4112780336.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_master['high_ticket_volume'] = df_master['high_ticket_volume'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# Merge usage summary with customers\n",
    "df_master = df_customers.merge(usage_summary, on='customer_id', how='left')\n",
    "\n",
    "# Merge ticket summary\n",
    "df_master = df_master.merge(ticket_summary, on='customer_id', how='left')\n",
    "\n",
    "# Fill nulls for customers with no usage/tickets\n",
    "# Fill usage-related nulls\n",
    "usage_cols = ['active_days', 'total_logins', 'avg_logins_per_day', 'std_logins',\n",
    "              'total_feature_events', 'avg_feature_events', 'total_session_minutes', \n",
    "              'avg_session_minutes', 'corrupted_records_count',\n",
    "              'active_days_clean', 'total_logins_clean', 'avg_logins_per_day_clean',\n",
    "              'total_session_minutes_clean']\n",
    "for col in usage_cols:\n",
    "    if col in df_master.columns:\n",
    "        df_master[col] = df_master[col].fillna(0)\n",
    "\n",
    "df_master['never_logged_in'] = df_master['never_logged_in'].fillna(True)\n",
    "df_master['low_engagement'] = df_master['low_engagement'].fillna(True)\n",
    "df_master['potential_outlier'] = df_master['potential_outlier'].fillna(False)\n",
    "\n",
    "# Fill ticket-related nulls\n",
    "ticket_cols = ['total_tickets', 'dashboard_issue_count', 'product_bug_count', \n",
    "               'onboarding_issue_count', 'avg_sentiment', 'avg_first_response_hours',\n",
    "               'avg_resolution_hours', 'resolution_rate']\n",
    "for col in ticket_cols:\n",
    "    if col in df_master.columns:\n",
    "        df_master[col] = df_master[col].fillna(0)\n",
    "\n",
    "df_master['primary_complaint_type'] = df_master['primary_complaint_type'].fillna('No_Tickets')\n",
    "df_master['high_ticket_volume'] = df_master['high_ticket_volume'].fillna(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b525e14",
   "metadata": {},
   "source": [
    "### Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17333e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.to_csv('../data/rigorous_cleaned_master_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
